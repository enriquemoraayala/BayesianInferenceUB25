{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Session 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thomas Bayes\n",
    "\n",
    "Thomas Bayes, a mathematician, was born in London, England in 1702 and died in Tunbridge Wells, Kent, England on April 17, 1761. His father, Joshua Bayes, was one of the first six non-conformist (Presbyterian) ministers to be ordained in England. His family moved to London when his father Joshua was appointed assistant at St Thomas. He was the oldest of seven siblings. Possibly De Moivre, author of the famous book The Doctrine of Probabilities, was his private tutor as it is known that he was teaching in London at that time. In 1719, he enrolled at Edinburgh University where he studied logic and theology. Non-conformist students were not allowed at Oxford or Cambridge. Like his father, Bayes was ordained as a dissenting minister, and in 1731 he became a reverend of the Presbyterian church in Tunbridge Wells; apparently he tried to retire in 1749, but continued to serve until 1752, and remained there until his death.\n",
    "\n",
    "![Thomas Bayes](./images/bayes_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the time of the Reverends Thomas Bayes and Richard Price, in the 1740s, a harsh controversy had erupted about the **improbability of Christian miracles**. The question was whether there were any clues in the natural world to help us reach rational conclusions about a Creator God, what in the 18th century was known as the cause or **the first cause**.\n",
    "\n",
    "We don't know whether Bayes was trying to prove the existence of God as a cause. But we do know that he tried to mathematically confront the problem of cause and effect. However, Bayes didn't trust his theorem enough to publish it. He archived the theorem in his notebook, and it wasn't discovered until ten or fifteen years after his death. In his will, he left Price 100 pounds and a request: to please look at his unpublished writings.\n",
    "\n",
    "While reviewing it, Price became convinced that the theorem would help prove the existence of God as a cause. After two years of editing Bayes' theorem, he published it in an English magazine that, unfortunately, few mathematicians read. \n",
    "\n",
    "A few years later, in 1774, a great French mathematician, **Pierre Simon Laplace**, discovered the rule independently of Bayes and Price. Laplace called it the probability of causes. Unlike Bayes and Price, Laplace was the quintessential professional scientist. He mathematized all known science of his time and spent forty years intermittently developing what we know as **Bayes' rule** (Laplace, 1812). In fact, up until about fifty years ago, Bayes' rule was considered part of Laplace's work. By right, we should still know it as Laplace's rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But... what is Bayesian inference?\n",
    "\n",
    "mmmm... the question is complex. Let's start with a couple of examples and definitions:\n",
    "\n",
    "**Definition**: Bayesian Inference is a statistical framework that focuses on updating our understanding (or “belief”) about uncertain parameters in light of new data. It is grounded in Bayes’ Theorem, which provides a systematic way to revise probabilities (or probability distributions) as additional evidence is introduced. Rather than relying solely on fixed estimates, Bayesian methods treat model parameters as random variables with prior probability distributions; these priors get updated to posterior distributions once data is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wait wait...inference?\n",
    "\n",
    "The main reason to use **statistical inference** is to make informed decisions or draw conclusions about a population or a process when it is impractical or impossible to observe all data. Statistical inference provides tools to analyze sample data and generalize findings to the broader population, all while quantifying uncertainty. This is particularly valuable in real-world scenarios where complete information is rarely available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/gaussian_curve_01.png\" width=\"600\">()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian, Frequentist... I don't understand nothing\n",
    "\n",
    "I can imagine. Let's begin with some differences:\n",
    "\n",
    "**Frequentist Inference** focuses on the idea that probabilities represent long-run frequencies of events. Under this framework, model parameters (e.g., a population mean) are viewed as fixed but unknown quantities, and the data we observe are considered random. Inference techniques—such as hypothesis testing or confidence intervals—are based on the sampling distribution of the data. For example, a frequentist might say, “If we repeated this experiment infinitely many times, 95% of the confidence intervals constructed would contain the true parameter.”\n",
    "\n",
    "**Bayesian Inference**, on the other hand, treats parameters themselves as random variables, assigning them probability distributions to capture one’s uncertainty about their true values. Bayes’ Theorem provides a systematic way to update these prior beliefs in light of new data, resulting in a posterior distribution that reflects both prior information and the observed evidence. This allows for statements like, “There is a 95% probability that the true parameter lies within this credible interval,” which has a more direct probabilistic interpretation of uncertainty.\n",
    "\n",
    "\n",
    "### Example of Frequentist Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Formulate the Hypotheses**\n",
    "- **Null Hypothesis ($H_0$)**: This is the default assumption or status quo. It represents no effect, no difference, or the parameter being equal to a specific value (e.g., $H_0: \\mu = 0$).\n",
    "- **Alternative Hypothesis ($H_a$)**: This represents the claim you want to test or prove (e.g., $H_a: \\mu \\neq 0$).\n",
    "\n",
    "The alternative hypothesis can be:\n",
    "  - **Two-tailed**: Tests for any difference ($H_a: \\mu \\neq 0$).\n",
    "  - **One-tailed**: Tests for a specific direction ($H_a: \\mu > 0$ or $H_a: \\mu < 0$).\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Choose the Significance Level ($\\alpha$)**\n",
    "- Set the threshold for rejecting $H_0$, typically $\\alpha = 0.05$ (5%). This represents the probability of rejecting $H_0$ when it is actually true (Type I error).\n",
    "\n",
    "\n",
    "\n",
    "#### **3. Collect and Summarize the Data**\n",
    "- Compute the relevant **sample statistic** (e.g., sample mean, proportion, or standard deviation) from your data.\n",
    "\n",
    "\n",
    "#### **4. Calculate the Test Statistic**\n",
    "- The test statistic quantifies how far your sample statistic is from the null hypothesis, relative to its standard error. Common test statistics include:\n",
    "  - **t-statistic**: For small sample sizes or unknown population standard deviation.\n",
    "  - **z-statistic**: For large sample sizes with a known population standard deviation.\n",
    "\n",
    "For example, the test statistic for a single-sample t-test is:\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
    "$$\n",
    "where:\n",
    "- $\\bar{x}$ is the sample mean,\n",
    "- $\\mu_0$ is the hypothesized population mean under $H_0$,\n",
    "- $s$ is the sample standard deviation,\n",
    "- $n$ is the sample size.\n",
    "\n",
    "\n",
    "\n",
    "#### **5. Determine the p-value**\n",
    "- The **p-value** is the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming $H_0$ is true.\n",
    "  - A **small p-value** (typically $p < \\alpha$) indicates strong evidence against $H_0$.\n",
    "  - A **large p-value** ($p \\geq \\alpha$) suggests insufficient evidence to reject $H_0$.\n",
    "\n",
    "\n",
    "\n",
    "#### **6. Make a Decision**\n",
    "- **Reject $H_0$** if the p-value is less than $\\alpha$: This means there is sufficient evidence to support $H_a$.\n",
    "- **Fail to Reject $H_0$** if the p-value is greater than or equal to $\\alpha$: This means the data do not provide strong enough evidence to support $H_a$.\n",
    "\n",
    "\n",
    "\n",
    "#### **7. Report Results**\n",
    "- State your conclusion in the context of the problem, including:\n",
    "  - The test statistic and p-value.\n",
    "  - The significance level ($\\alpha$).\n",
    "  - Whether you rejected or failed to reject $H_0$.\n",
    "  - A real-world interpretation of the findings.\n",
    "\n",
    "\n",
    "\n",
    "#### **Example**\n",
    "Suppose you have a sample of weights from a new product ($n = 30$) and want to test if the mean weight differs from 10 kg:\n",
    "1. $H_0: \\mu = 10$, $H_a: \\mu \\neq 10$.\n",
    "2. Set $\\alpha = 0.05$.\n",
    "3. Compute the sample mean ($\\bar{x} = 10.4$) and standard deviation ($s = 0.6$).\n",
    "4. Calculate the t-statistic:\n",
    "$$\n",
    "   t = \\frac{10.4 - 10}{0.6 / \\sqrt{30}} \\approx 3.65\n",
    "$$\n",
    "5. Find the p-value for $t = 3.65$ with $df = 29$.\n",
    "\n",
    "If you calculate a $t$-statistic of 3.65 with 29 degrees of freedom:\n",
    "- The p-value for a two-tailed test would be approximately:\n",
    "$p = 2 \\times (1 - P(T < 3.65)) \\approx 0.001$\n",
    "\n",
    "6. Compare the p-value to $\\alpha$. If $p < 0.05$, reject $H_0$.\n",
    "7. Conclude: \"There is evidence that the mean weight is significantly different from 10 kg.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.0010253327283737956\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Inputs\n",
    "t_stat = 3.65  # Test statistic\n",
    "df = 29  # Degrees of freedom\n",
    "\n",
    "# Two-tailed p-value\n",
    "p_value = 2 * (1 - t.cdf(abs(t_stat), df))\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
